{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ky84E7MWGL05"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "  def __init__(self, embed_dim, num_head, ff_dim, rate  = 0.1):\n",
        "\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(num_heads = num_heads , key_dim = embed_dim)\n",
        "\n",
        "    self.ffn = Sequential(\n",
        "        [Dense(ff_dim, activation = 'relu'), Dense(embed_dim)],\n",
        "    )\n",
        "\n",
        "    self.layernorm1 = LayerNormalization(epsilon = 1e-6)\n",
        "    self.layernorm2 = LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    self.dropout1 = Dropout(rate)\n",
        "    self.dropout2 = Dropout(rate)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    attn_output = self.att(inputs, inputs)\n",
        "\n",
        "    attn_output = self.dropout1(attn_output, training = training)\n",
        "\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "    ffn_output = self.ffn(out1)\n",
        "\n",
        "    ffn_output = self.dropout2(ffn_output, training = training)\n",
        "\n",
        "    return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "fcKd72L_HACt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Embedding Layer"
      ],
      "metadata": {
        "id": "sTCaUL45MEEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(Layer):\n",
        "\n",
        "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "\n",
        "    super().__init__()\n",
        "    self.token_emb = Embedding(input_dim= vocab_size, output_dim=embed_dim)\n",
        "\n",
        "    self.pos_emb = Embedding(input_dim= maxlen, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    maxlen = tf.shape(x)[-1]\n",
        "\n",
        "    positions = tf.range(start = 0, limit = maxlen, delta = 1)\n",
        "\n",
        "    postions = self.pos_emb(positions)\n",
        "\n",
        "    x = self.token_emb(x)\n",
        "\n",
        "    return x + postions"
      ],
      "metadata": {
        "id": "RRjAmmASMZwc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000 # Only consider the top 20k words\n",
        "maxlen = 200\n",
        "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words = vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = pad_sequences(x_train, maxlen = maxlen)\n",
        "x_val = pad_sequences(x_val, maxlen = maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MDU7xouPHFx",
        "outputId": "63eaa4fb-273f-4253-af82-04a611f6d854"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpXRL5UYnBnQ",
        "outputId": "a8755307-42dc-488d-a045-824d0f6283ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    5,    25,   100,    43,   838,   112,    50,   670,     2,\n",
              "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
              "         167,     2,   336,   385,    39,     4,   172,  4536,  1111,\n",
              "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
              "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
              "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
              "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
              "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
              "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
              "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
              "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
              "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
              "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
              "         107,   117,  5952,    15,   256,     4,     2,     7,  3766,\n",
              "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
              "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
              "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
              "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
              "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
              "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
              "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
              "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
              "         178,    32], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsH1ly4WPG36",
        "outputId": "8645d904-10ce-4cc8-86ad-db2d413d95ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "\n",
        "inputs = Input(shape = (maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation = 'relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(2, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = outputs)"
      ],
      "metadata": {
        "id": "nBJDAafRRByR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size = 32, epochs = 10, validation_data = (x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k5Sh_JSepcz",
        "outputId": "4d1f4e15-7d00-47fa-f7f8-3e791e70e4e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 48s 56ms/step - loss: 0.0201 - accuracy: 0.9943 - val_loss: 0.9688 - val_accuracy: 0.8330\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 1.0573 - val_accuracy: 0.8263\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 1.3038 - val_accuracy: 0.8187\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 1.2250 - val_accuracy: 0.8134\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 12s 15ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 1.3133 - val_accuracy: 0.8199\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 12s 15ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 1.6591 - val_accuracy: 0.8196\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.0056 - accuracy: 0.9979 - val_loss: 1.6736 - val_accuracy: 0.8184\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 1.4272 - val_accuracy: 0.8222\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 12s 15ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 1.8920 - val_accuracy: 0.8146\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 1.5963 - val_accuracy: 0.8193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "och_aMeHkB9W",
        "outputId": "54bf1ef9-e47a-4e73-c890-7432d920a2ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 1.5963 - accuracy: 0.8193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5963256359100342, 0.8192800283432007]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(text):\n",
        "  text = pad_sequences(text, maxlen = maxlen)\n",
        "  prediction = model.predict(text)\n",
        "  return prediction.argmax(axis = 1)\n"
      ],
      "metadata": {
        "id": "oyE8E4YWlx80"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"It is no wonder that the film has such a high rating, it is quite literally breathtaking. What can I say that hasn't said before? Not much, it's the story, the acting, the premise, but most of all, this movie is about how it makes you feel. Sometimes you watch a film, and can't remember it days later, this film loves with you, once you've seen it, you don't forget. The ultimate story of friendship, of hope, and of life, and overcoming adversity. I understand why so many class this as the best film of all time, it isn't mine, but I get it. If you haven't seen it, or haven't seen it for some time, you need to watch it, it's amazing. 10/10.\"\"\""
      ],
      "metadata": {
        "id": "Zs6mmIBlmxQe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "XLKcryUwmu-_",
        "outputId": "3f52dac2-bcd4-4375-b1e3-03ce5a52f2ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2cdd15527bb3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-776a2e16cb97>\u001b[0m in \u001b[0;36mpredict_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/data_utils.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'I'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words = vocab_size)\n",
        "x_train[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bpuCoWUnJJg",
        "outputId": "9278181f-0546-4335-92e6-78e5df09bd15"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 2,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 2,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 2,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 2,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}