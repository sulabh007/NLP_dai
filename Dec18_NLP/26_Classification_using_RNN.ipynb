{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 12:43:55.231674: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 12:43:55.399300: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 12:43:55.585346: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 12:43:55.585434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 12:43:55.603451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 12:43:55.638431: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 12:43:55.640017: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 12:44:05.733403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences= []\n",
    "training_labels = []\n",
    "testing_sentences= []\n",
    "testing_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(str(l.numpy()))\n",
    "\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(str(l.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"This Alec Guinness starrer is a very good fun political satire of corporate industry, and a light eccentric character study as well.<br /><br />The pacing is a bit slow for a comedy, and none of it is really rolling-on-the-floor type funny, except perhaps the sound effects for the experiments. But it does have its amusing moments, and it is very deft in its execution. The big explosions segment is probably the most farcical element.<br /><br />The union procedures are quite droll, very reminiscent of I\\'M ALL RIGHT JACK; especially the feminine socialist with a light romantic crush on Guinness\\' character. The political machinations actually carry the story. Ernest Thesigner is very notable as a heavy.<br /><br />I don\\'t think this one works quite as well as THE LADYKILLERS, or KIND HEARTS AND CORONETS; but even light Ealing comedy is better than nothing.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_sentences), len(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.Series(training_labels)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 12500, '1': 12500})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86538"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_index;\n",
    "len( word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
    "testing_sentences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  873,  144,    9],\n",
       "       [   0,    0,    0, ...,   31,   30,   46],\n",
       "       [6174,    1, 4915, ...,    8, 6175,   46],\n",
       "       ...,\n",
       "       [7628,   36,   10, ...,  167,    5,   28],\n",
       "       [2676,   10,  215, ...,    1,   88,   10],\n",
       "       [3874,    4,   30, ...,    5,  994, 5125]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'br': 7,\n",
       " 'in': 8,\n",
       " 'it': 9,\n",
       " 'i': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'for': 15,\n",
       " 'with': 16,\n",
       " 'movie': 17,\n",
       " 'but': 18,\n",
       " 'film': 19,\n",
       " \"'s\": 20,\n",
       " 'on': 21,\n",
       " 'you': 22,\n",
       " 'not': 23,\n",
       " 'are': 24,\n",
       " 'his': 25,\n",
       " 'he': 26,\n",
       " 'have': 27,\n",
       " 'be': 28,\n",
       " 'one': 29,\n",
       " 'all': 30,\n",
       " 'at': 31,\n",
       " 'by': 32,\n",
       " 'they': 33,\n",
       " 'an': 34,\n",
       " 'who': 35,\n",
       " 'so': 36,\n",
       " 'from': 37,\n",
       " 'like': 38,\n",
       " 'her': 39,\n",
       " \"'t\": 40,\n",
       " 'or': 41,\n",
       " 'just': 42,\n",
       " 'there': 43,\n",
       " 'about': 44,\n",
       " 'out': 45,\n",
       " \"'\": 46,\n",
       " 'has': 47,\n",
       " 'if': 48,\n",
       " 'some': 49,\n",
       " 'what': 50,\n",
       " 'good': 51,\n",
       " 'more': 52,\n",
       " 'very': 53,\n",
       " 'when': 54,\n",
       " 'she': 55,\n",
       " 'up': 56,\n",
       " 'can': 57,\n",
       " 'b': 58,\n",
       " 'time': 59,\n",
       " 'no': 60,\n",
       " 'even': 61,\n",
       " 'my': 62,\n",
       " 'would': 63,\n",
       " 'which': 64,\n",
       " 'story': 65,\n",
       " 'only': 66,\n",
       " 'really': 67,\n",
       " 'see': 68,\n",
       " 'their': 69,\n",
       " 'had': 70,\n",
       " 'were': 71,\n",
       " 'me': 72,\n",
       " 'well': 73,\n",
       " 'we': 74,\n",
       " 'than': 75,\n",
       " 'much': 76,\n",
       " 'been': 77,\n",
       " 'get': 78,\n",
       " 'bad': 79,\n",
       " 'will': 80,\n",
       " 'people': 81,\n",
       " 'do': 82,\n",
       " 'also': 83,\n",
       " 'other': 84,\n",
       " 'into': 85,\n",
       " 'because': 86,\n",
       " 'great': 87,\n",
       " 'first': 88,\n",
       " 'him': 89,\n",
       " 'how': 90,\n",
       " 'most': 91,\n",
       " 'made': 92,\n",
       " 'its': 93,\n",
       " 'then': 94,\n",
       " 'way': 95,\n",
       " 'make': 96,\n",
       " 'them': 97,\n",
       " 'could': 98,\n",
       " 'too': 99,\n",
       " 'movies': 100,\n",
       " 'any': 101,\n",
       " \"it's\": 102,\n",
       " 'after': 103,\n",
       " 'think': 104,\n",
       " 'characters': 105,\n",
       " 'watch': 106,\n",
       " 'two': 107,\n",
       " 'films': 108,\n",
       " 'character': 109,\n",
       " 'seen': 110,\n",
       " 'many': 111,\n",
       " 'life': 112,\n",
       " 'being': 113,\n",
       " 'plot': 114,\n",
       " 'acting': 115,\n",
       " 'never': 116,\n",
       " 'little': 117,\n",
       " 'love': 118,\n",
       " 'best': 119,\n",
       " 'where': 120,\n",
       " 'over': 121,\n",
       " 'did': 122,\n",
       " 'show': 123,\n",
       " 'know': 124,\n",
       " 'off': 125,\n",
       " 'ever': 126,\n",
       " 'does': 127,\n",
       " 'man': 128,\n",
       " 'better': 129,\n",
       " 'your': 130,\n",
       " 'here': 131,\n",
       " 'end': 132,\n",
       " 'still': 133,\n",
       " 'these': 134,\n",
       " 'say': 135,\n",
       " 'scene': 136,\n",
       " 'why': 137,\n",
       " 'while': 138,\n",
       " 'scenes': 139,\n",
       " 'go': 140,\n",
       " 'such': 141,\n",
       " 'something': 142,\n",
       " 'should': 143,\n",
       " 'through': 144,\n",
       " 'back': 145,\n",
       " 'don': 146,\n",
       " 'real': 147,\n",
       " 'those': 148,\n",
       " 'now': 149,\n",
       " 'watching': 150,\n",
       " 'though': 151,\n",
       " 'thing': 152,\n",
       " 'old': 153,\n",
       " 'years': 154,\n",
       " 'actors': 155,\n",
       " 'work': 156,\n",
       " 'director': 157,\n",
       " 'new': 158,\n",
       " 'another': 159,\n",
       " 'before': 160,\n",
       " 'funny': 161,\n",
       " 'nothing': 162,\n",
       " 'actually': 163,\n",
       " 'makes': 164,\n",
       " 'look': 165,\n",
       " 'find': 166,\n",
       " 'going': 167,\n",
       " 'few': 168,\n",
       " 'same': 169,\n",
       " 'part': 170,\n",
       " 'again': 171,\n",
       " 'lot': 172,\n",
       " \"don't\": 173,\n",
       " 'every': 174,\n",
       " '10': 175,\n",
       " 'cast': 176,\n",
       " 'us': 177,\n",
       " 'world': 178,\n",
       " 'quite': 179,\n",
       " 'down': 180,\n",
       " 'want': 181,\n",
       " 'things': 182,\n",
       " 'pretty': 183,\n",
       " 'young': 184,\n",
       " 'seems': 185,\n",
       " 'around': 186,\n",
       " 'got': 187,\n",
       " 'horror': 188,\n",
       " 'however': 189,\n",
       " 'fact': 190,\n",
       " 'take': 191,\n",
       " 'big': 192,\n",
       " 'enough': 193,\n",
       " 'long': 194,\n",
       " 'thought': 195,\n",
       " 'both': 196,\n",
       " 'series': 197,\n",
       " 'between': 198,\n",
       " 'may': 199,\n",
       " 'give': 200,\n",
       " 'original': 201,\n",
       " 'own': 202,\n",
       " 'action': 203,\n",
       " 'right': 204,\n",
       " 'without': 205,\n",
       " 'times': 206,\n",
       " 'always': 207,\n",
       " 'comedy': 208,\n",
       " 'point': 209,\n",
       " 'must': 210,\n",
       " 'gets': 211,\n",
       " 'role': 212,\n",
       " 'come': 213,\n",
       " 'family': 214,\n",
       " 'saw': 215,\n",
       " 'almost': 216,\n",
       " 'interesting': 217,\n",
       " 'least': 218,\n",
       " 'done': 219,\n",
       " 'whole': 220,\n",
       " 'bit': 221,\n",
       " 'music': 222,\n",
       " 'script': 223,\n",
       " 'guy': 224,\n",
       " 'far': 225,\n",
       " 'xc2': 226,\n",
       " 'anything': 227,\n",
       " 'making': 228,\n",
       " 'minutes': 229,\n",
       " 'feel': 230,\n",
       " 'last': 231,\n",
       " 'might': 232,\n",
       " 'performance': 233,\n",
       " 'since': 234,\n",
       " '2': 235,\n",
       " 'probably': 236,\n",
       " 'girl': 237,\n",
       " 'kind': 238,\n",
       " 'am': 239,\n",
       " 'away': 240,\n",
       " 'tv': 241,\n",
       " 'yet': 242,\n",
       " 'woman': 243,\n",
       " 'rather': 244,\n",
       " 'day': 245,\n",
       " \"'ve\": 246,\n",
       " 'worst': 247,\n",
       " 'fun': 248,\n",
       " 'sure': 249,\n",
       " 'hard': 250,\n",
       " \"'m\": 251,\n",
       " 'doesn': 252,\n",
       " 'anyone': 253,\n",
       " 'played': 254,\n",
       " \"b'i\": 255,\n",
       " 'each': 256,\n",
       " 'found': 257,\n",
       " 'xc3': 258,\n",
       " 'especially': 259,\n",
       " 'course': 260,\n",
       " 'our': 261,\n",
       " 'having': 262,\n",
       " 'believe': 263,\n",
       " 'comes': 264,\n",
       " 'screen': 265,\n",
       " 'looking': 266,\n",
       " 'although': 267,\n",
       " 'trying': 268,\n",
       " 'goes': 269,\n",
       " 'set': 270,\n",
       " 'looks': 271,\n",
       " 'place': 272,\n",
       " 'book': 273,\n",
       " 'different': 274,\n",
       " 'put': 275,\n",
       " 'money': 276,\n",
       " 'actor': 277,\n",
       " 'ending': 278,\n",
       " 'maybe': 279,\n",
       " 'year': 280,\n",
       " 'sense': 281,\n",
       " 'reason': 282,\n",
       " 'true': 283,\n",
       " 'everything': 284,\n",
       " 'dvd': 285,\n",
       " 'shows': 286,\n",
       " 'once': 287,\n",
       " 'didn': 288,\n",
       " 'someone': 289,\n",
       " 'three': 290,\n",
       " 'worth': 291,\n",
       " 'job': 292,\n",
       " \"'re\": 293,\n",
       " 'main': 294,\n",
       " 'together': 295,\n",
       " 'play': 296,\n",
       " 'watched': 297,\n",
       " 'american': 298,\n",
       " 'plays': 299,\n",
       " '1': 300,\n",
       " 'effects': 301,\n",
       " 'later': 302,\n",
       " 'said': 303,\n",
       " 'takes': 304,\n",
       " 'instead': 305,\n",
       " 'audience': 306,\n",
       " 'seem': 307,\n",
       " 'john': 308,\n",
       " 'beautiful': 309,\n",
       " 'everyone': 310,\n",
       " 'himself': 311,\n",
       " 'version': 312,\n",
       " 'house': 313,\n",
       " 'high': 314,\n",
       " 'night': 315,\n",
       " 'during': 316,\n",
       " 'left': 317,\n",
       " \"i'm\": 318,\n",
       " 'special': 319,\n",
       " 'wife': 320,\n",
       " 'seeing': 321,\n",
       " 'half': 322,\n",
       " 'let': 323,\n",
       " 'star': 324,\n",
       " 'father': 325,\n",
       " 'excellent': 326,\n",
       " 'shot': 327,\n",
       " 'war': 328,\n",
       " \"didn't\": 329,\n",
       " 'idea': 330,\n",
       " 'black': 331,\n",
       " 'nice': 332,\n",
       " 'less': 333,\n",
       " 'mind': 334,\n",
       " 'else': 335,\n",
       " 'read': 336,\n",
       " \"doesn't\": 337,\n",
       " 'second': 338,\n",
       " 'simply': 339,\n",
       " 'fan': 340,\n",
       " 'help': 341,\n",
       " 'death': 342,\n",
       " 'poor': 343,\n",
       " 'completely': 344,\n",
       " '3': 345,\n",
       " 'men': 346,\n",
       " 'used': 347,\n",
       " 'home': 348,\n",
       " 'either': 349,\n",
       " 'short': 350,\n",
       " 'hollywood': 351,\n",
       " 'line': 352,\n",
       " 'dead': 353,\n",
       " 'given': 354,\n",
       " 'top': 355,\n",
       " 'budget': 356,\n",
       " 'kids': 357,\n",
       " 'try': 358,\n",
       " \"b'this\": 359,\n",
       " 'performances': 360,\n",
       " 'wrong': 361,\n",
       " 'classic': 362,\n",
       " 'enjoy': 363,\n",
       " 'boring': 364,\n",
       " 'need': 365,\n",
       " 'rest': 366,\n",
       " 'use': 367,\n",
       " 'low': 368,\n",
       " 'women': 369,\n",
       " 'production': 370,\n",
       " 'isn': 371,\n",
       " 'friends': 372,\n",
       " 'until': 373,\n",
       " 'camera': 374,\n",
       " 'along': 375,\n",
       " 'full': 376,\n",
       " 'truly': 377,\n",
       " 'video': 378,\n",
       " 'awful': 379,\n",
       " 'next': 380,\n",
       " 'tell': 381,\n",
       " 'couple': 382,\n",
       " 'start': 383,\n",
       " 'stupid': 384,\n",
       " 'remember': 385,\n",
       " 'mean': 386,\n",
       " 'sex': 387,\n",
       " 'stars': 388,\n",
       " 'perhaps': 389,\n",
       " 'came': 390,\n",
       " 'recommend': 391,\n",
       " 'moments': 392,\n",
       " 'episode': 393,\n",
       " 'wonderful': 394,\n",
       " 'school': 395,\n",
       " 'understand': 396,\n",
       " 'small': 397,\n",
       " 'face': 398,\n",
       " 'terrible': 399,\n",
       " 'playing': 400,\n",
       " \"i've\": 401,\n",
       " 'getting': 402,\n",
       " 'written': 403,\n",
       " \"'ll\": 404,\n",
       " 'early': 405,\n",
       " 'doing': 406,\n",
       " 'often': 407,\n",
       " 'name': 408,\n",
       " 'keep': 409,\n",
       " 'perfect': 410,\n",
       " 'style': 411,\n",
       " 'human': 412,\n",
       " 'others': 413,\n",
       " 'gives': 414,\n",
       " 'definitely': 415,\n",
       " 'person': 416,\n",
       " 'itself': 417,\n",
       " 'lines': 418,\n",
       " 'live': 419,\n",
       " 'become': 420,\n",
       " \"can't\": 421,\n",
       " 'dialogue': 422,\n",
       " 'head': 423,\n",
       " 'lost': 424,\n",
       " 'piece': 425,\n",
       " 'case': 426,\n",
       " 'felt': 427,\n",
       " 'finally': 428,\n",
       " 'boy': 429,\n",
       " 'supposed': 430,\n",
       " 'liked': 431,\n",
       " 'title': 432,\n",
       " 'yes': 433,\n",
       " 'white': 434,\n",
       " 'cinema': 435,\n",
       " 'picture': 436,\n",
       " 'against': 437,\n",
       " 'absolutely': 438,\n",
       " 'mother': 439,\n",
       " 'sort': 440,\n",
       " 'worse': 441,\n",
       " 'entire': 442,\n",
       " 'certainly': 443,\n",
       " 'went': 444,\n",
       " 'waste': 445,\n",
       " 'problem': 446,\n",
       " 'hope': 447,\n",
       " 'entertaining': 448,\n",
       " 'evil': 449,\n",
       " 'mr': 450,\n",
       " 'overall': 451,\n",
       " 'called': 452,\n",
       " 'children': 453,\n",
       " 'loved': 454,\n",
       " 'based': 455,\n",
       " 'killer': 456,\n",
       " 'several': 457,\n",
       " 'friend': 458,\n",
       " 'fans': 459,\n",
       " \"that's\": 460,\n",
       " 'drama': 461,\n",
       " \"isn't\": 462,\n",
       " 'beginning': 463,\n",
       " 'lives': 464,\n",
       " 'direction': 465,\n",
       " '5': 466,\n",
       " 'care': 467,\n",
       " 'becomes': 468,\n",
       " 'already': 469,\n",
       " \"'d\": 470,\n",
       " 'laugh': 471,\n",
       " 'example': 472,\n",
       " 'oh': 473,\n",
       " 'dark': 474,\n",
       " 'under': 475,\n",
       " 'seemed': 476,\n",
       " 'throughout': 477,\n",
       " 'turn': 478,\n",
       " '4': 479,\n",
       " 'wanted': 480,\n",
       " 'unfortunately': 481,\n",
       " 'x96': 482,\n",
       " 'son': 483,\n",
       " 'despite': 484,\n",
       " 'history': 485,\n",
       " 'fine': 486,\n",
       " 'final': 487,\n",
       " 'sound': 488,\n",
       " 'heart': 489,\n",
       " 'amazing': 490,\n",
       " 'guess': 491,\n",
       " 'lead': 492,\n",
       " 'humor': 493,\n",
       " 'totally': 494,\n",
       " 'michael': 495,\n",
       " 'writing': 496,\n",
       " 'quality': 497,\n",
       " \"there's\": 498,\n",
       " 'guys': 499,\n",
       " 'close': 500,\n",
       " 'wants': 501,\n",
       " 'child': 502,\n",
       " 'behind': 503,\n",
       " 'works': 504,\n",
       " 'tries': 505,\n",
       " 'side': 506,\n",
       " 'art': 507,\n",
       " 'game': 508,\n",
       " 'past': 509,\n",
       " 'town': 510,\n",
       " 'days': 511,\n",
       " 'able': 512,\n",
       " 'flick': 513,\n",
       " 'hand': 514,\n",
       " 'turns': 515,\n",
       " 'genre': 516,\n",
       " 'act': 517,\n",
       " 'enjoyed': 518,\n",
       " 'viewer': 519,\n",
       " 'favorite': 520,\n",
       " 'kill': 521,\n",
       " 'car': 522,\n",
       " 'soon': 523,\n",
       " 'starts': 524,\n",
       " 'gave': 525,\n",
       " 'run': 526,\n",
       " 'sometimes': 527,\n",
       " 'etc': 528,\n",
       " 'actress': 529,\n",
       " 'eyes': 530,\n",
       " 'late': 531,\n",
       " 'directed': 532,\n",
       " 'horrible': 533,\n",
       " 'parts': 534,\n",
       " 'brilliant': 535,\n",
       " 'wasn': 536,\n",
       " 'girls': 537,\n",
       " 'themselves': 538,\n",
       " 'hour': 539,\n",
       " 'self': 540,\n",
       " 'stories': 541,\n",
       " 'thinking': 542,\n",
       " 'expect': 543,\n",
       " 'city': 544,\n",
       " 'stuff': 545,\n",
       " 'kid': 546,\n",
       " 'god': 547,\n",
       " 'won': 548,\n",
       " 'blood': 549,\n",
       " 'obviously': 550,\n",
       " 'decent': 551,\n",
       " 'voice': 552,\n",
       " 'highly': 553,\n",
       " 'myself': 554,\n",
       " 'feeling': 555,\n",
       " 'fight': 556,\n",
       " 'today': 557,\n",
       " 'matter': 558,\n",
       " 'except': 559,\n",
       " 'writer': 560,\n",
       " \"wasn't\": 561,\n",
       " 'slow': 562,\n",
       " 'type': 563,\n",
       " \"he's\": 564,\n",
       " 'anyway': 565,\n",
       " 'roles': 566,\n",
       " 'age': 567,\n",
       " 'killed': 568,\n",
       " 'heard': 569,\n",
       " 'says': 570,\n",
       " 'moment': 571,\n",
       " 'daughter': 572,\n",
       " 'took': 573,\n",
       " 'leave': 574,\n",
       " 'strong': 575,\n",
       " 'cannot': 576,\n",
       " 'violence': 577,\n",
       " 's': 578,\n",
       " 'hit': 579,\n",
       " 'police': 580,\n",
       " 'stop': 581,\n",
       " 'happens': 582,\n",
       " 'particularly': 583,\n",
       " 'known': 584,\n",
       " 'happened': 585,\n",
       " 'involved': 586,\n",
       " 'brother': 587,\n",
       " 'obvious': 588,\n",
       " 'extremely': 589,\n",
       " 'chance': 590,\n",
       " 'told': 591,\n",
       " 'living': 592,\n",
       " 'experience': 593,\n",
       " 'lack': 594,\n",
       " 'coming': 595,\n",
       " 'alone': 596,\n",
       " 'james': 597,\n",
       " 'including': 598,\n",
       " 'murder': 599,\n",
       " 'attempt': 600,\n",
       " 'happen': 601,\n",
       " 'please': 602,\n",
       " 'wonder': 603,\n",
       " 'gore': 604,\n",
       " 'crap': 605,\n",
       " 'ago': 606,\n",
       " 'group': 607,\n",
       " 'complete': 608,\n",
       " 'none': 609,\n",
       " 'interest': 610,\n",
       " 'score': 611,\n",
       " 'cut': 612,\n",
       " 'simple': 613,\n",
       " 'hell': 614,\n",
       " 'save': 615,\n",
       " 'hero': 616,\n",
       " 'looked': 617,\n",
       " 'number': 618,\n",
       " 'song': 619,\n",
       " 'career': 620,\n",
       " 'husband': 621,\n",
       " 'possible': 622,\n",
       " 'annoying': 623,\n",
       " 'shown': 624,\n",
       " 'sad': 625,\n",
       " 'exactly': 626,\n",
       " 'seriously': 627,\n",
       " 'running': 628,\n",
       " 'musical': 629,\n",
       " 'serious': 630,\n",
       " 'yourself': 631,\n",
       " 'david': 632,\n",
       " 'whose': 633,\n",
       " 'taken': 634,\n",
       " 'cinematography': 635,\n",
       " 'released': 636,\n",
       " 'ends': 637,\n",
       " 'scary': 638,\n",
       " 'hours': 639,\n",
       " 'reality': 640,\n",
       " 'english': 641,\n",
       " 'usually': 642,\n",
       " 'light': 643,\n",
       " 'opening': 644,\n",
       " 'jokes': 645,\n",
       " 'across': 646,\n",
       " 'body': 647,\n",
       " 'hilarious': 648,\n",
       " 'somewhat': 649,\n",
       " 'relationship': 650,\n",
       " 'started': 651,\n",
       " 'usual': 652,\n",
       " 'view': 653,\n",
       " 'cool': 654,\n",
       " 'level': 655,\n",
       " 'ridiculous': 656,\n",
       " 'change': 657,\n",
       " 'opinion': 658,\n",
       " 'happy': 659,\n",
       " 'wish': 660,\n",
       " 'king': 661,\n",
       " 'middle': 662,\n",
       " 'novel': 663,\n",
       " 'taking': 664,\n",
       " 'ones': 665,\n",
       " 'talking': 666,\n",
       " 'ok': 667,\n",
       " 'finds': 668,\n",
       " 'order': 669,\n",
       " 'shots': 670,\n",
       " 'documentary': 671,\n",
       " 'saying': 672,\n",
       " 'female': 673,\n",
       " 'huge': 674,\n",
       " 'room': 675,\n",
       " 'mostly': 676,\n",
       " 'power': 677,\n",
       " 'episodes': 678,\n",
       " 'robert': 679,\n",
       " 'important': 680,\n",
       " 'talent': 681,\n",
       " 'rating': 682,\n",
       " 'word': 683,\n",
       " 'turned': 684,\n",
       " 'strange': 685,\n",
       " 'major': 686,\n",
       " 'five': 687,\n",
       " 'modern': 688,\n",
       " 'call': 689,\n",
       " 'single': 690,\n",
       " 'disappointed': 691,\n",
       " 'country': 692,\n",
       " 'apparently': 693,\n",
       " 'events': 694,\n",
       " 'due': 695,\n",
       " 'songs': 696,\n",
       " 'attention': 697,\n",
       " 'earth': 698,\n",
       " '7': 699,\n",
       " 'four': 700,\n",
       " 'television': 701,\n",
       " 'jack': 702,\n",
       " 'knows': 703,\n",
       " 'comic': 704,\n",
       " 'supporting': 705,\n",
       " 'basically': 706,\n",
       " 'non': 707,\n",
       " 'clearly': 708,\n",
       " 'knew': 709,\n",
       " 'british': 710,\n",
       " 'future': 711,\n",
       " 'fast': 712,\n",
       " '8': 713,\n",
       " 'class': 714,\n",
       " 'cheap': 715,\n",
       " 'thriller': 716,\n",
       " 'silly': 717,\n",
       " 'problems': 718,\n",
       " 'tells': 719,\n",
       " 'easily': 720,\n",
       " 'miss': 721,\n",
       " 'local': 722,\n",
       " 'paul': 723,\n",
       " 'words': 724,\n",
       " 'sequence': 725,\n",
       " 'entertainment': 726,\n",
       " 'bring': 727,\n",
       " 'beyond': 728,\n",
       " 'rock': 729,\n",
       " 'upon': 730,\n",
       " 'oscar': 731,\n",
       " 'straight': 732,\n",
       " 'whether': 733,\n",
       " 'sets': 734,\n",
       " 'moving': 735,\n",
       " 'predictable': 736,\n",
       " 'similar': 737,\n",
       " 'romantic': 738,\n",
       " 'review': 739,\n",
       " 'falls': 740,\n",
       " 'george': 741,\n",
       " 'mystery': 742,\n",
       " 'needs': 743,\n",
       " 'appears': 744,\n",
       " 'enjoyable': 745,\n",
       " 'eye': 746,\n",
       " 'giving': 747,\n",
       " 'clich': 748,\n",
       " 'talk': 749,\n",
       " 'lady': 750,\n",
       " 'within': 751,\n",
       " 'richard': 752,\n",
       " \"b'\": 753,\n",
       " 'ten': 754,\n",
       " 'animation': 755,\n",
       " 'message': 756,\n",
       " 'theater': 757,\n",
       " 'near': 758,\n",
       " 'couldn': 759,\n",
       " 'above': 760,\n",
       " 'theme': 761,\n",
       " 'team': 762,\n",
       " 'nearly': 763,\n",
       " 'sequel': 764,\n",
       " 'points': 765,\n",
       " 'dull': 766,\n",
       " 'stand': 767,\n",
       " \"you're\": 768,\n",
       " 'bunch': 769,\n",
       " 'mention': 770,\n",
       " 'herself': 771,\n",
       " 'add': 772,\n",
       " 'feels': 773,\n",
       " 'release': 774,\n",
       " \"b'the\": 775,\n",
       " 'storyline': 776,\n",
       " 'ways': 777,\n",
       " 'sister': 778,\n",
       " 'surprised': 779,\n",
       " 'red': 780,\n",
       " 'named': 781,\n",
       " 'using': 782,\n",
       " 'york': 783,\n",
       " 'lots': 784,\n",
       " 'fantastic': 785,\n",
       " 'easy': 786,\n",
       " 'begins': 787,\n",
       " 'actual': 788,\n",
       " 't': 789,\n",
       " 'working': 790,\n",
       " 'effort': 791,\n",
       " 'lee': 792,\n",
       " 'tale': 793,\n",
       " 'die': 794,\n",
       " 'minute': 795,\n",
       " 'hate': 796,\n",
       " 'clear': 797,\n",
       " 'french': 798,\n",
       " 'stay': 799,\n",
       " 'feature': 800,\n",
       " 'elements': 801,\n",
       " 'viewers': 802,\n",
       " 'among': 803,\n",
       " '9': 804,\n",
       " 'follow': 805,\n",
       " 're': 806,\n",
       " 'comments': 807,\n",
       " 'showing': 808,\n",
       " 'avoid': 809,\n",
       " 'editing': 810,\n",
       " 'tried': 811,\n",
       " 'typical': 812,\n",
       " 'famous': 813,\n",
       " 'fall': 814,\n",
       " 'dialog': 815,\n",
       " 'tom': 816,\n",
       " 'season': 817,\n",
       " 'period': 818,\n",
       " 'check': 819,\n",
       " 'form': 820,\n",
       " 'soundtrack': 821,\n",
       " 'certain': 822,\n",
       " 'filmed': 823,\n",
       " 'sorry': 824,\n",
       " 'means': 825,\n",
       " 'buy': 826,\n",
       " 'material': 827,\n",
       " 'peter': 828,\n",
       " 'weak': 829,\n",
       " 'realistic': 830,\n",
       " 'figure': 831,\n",
       " 'crime': 832,\n",
       " 'doubt': 833,\n",
       " 'somehow': 834,\n",
       " 'gone': 835,\n",
       " 'general': 836,\n",
       " 'kept': 837,\n",
       " 'parents': 838,\n",
       " 'leads': 839,\n",
       " 'viewing': 840,\n",
       " 'space': 841,\n",
       " 'greatest': 842,\n",
       " 'suspense': 843,\n",
       " 'dance': 844,\n",
       " 'lame': 845,\n",
       " 'third': 846,\n",
       " 'brought': 847,\n",
       " 'atmosphere': 848,\n",
       " 'hear': 849,\n",
       " 'particular': 850,\n",
       " 'imagine': 851,\n",
       " 'sequences': 852,\n",
       " 'move': 853,\n",
       " 'whatever': 854,\n",
       " 'indeed': 855,\n",
       " 'rent': 856,\n",
       " 'eventually': 857,\n",
       " 'learn': 858,\n",
       " 'de': 859,\n",
       " 'deal': 860,\n",
       " 'reviews': 861,\n",
       " 'zombie': 862,\n",
       " 'wait': 863,\n",
       " 'japanese': 864,\n",
       " 'average': 865,\n",
       " 'sexual': 866,\n",
       " 'note': 867,\n",
       " 'premise': 868,\n",
       " 'forget': 869,\n",
       " 'poorly': 870,\n",
       " 'surprise': 871,\n",
       " 'believable': 872,\n",
       " 'sit': 873,\n",
       " 'disney': 874,\n",
       " 'stage': 875,\n",
       " 'nature': 876,\n",
       " 'possibly': 877,\n",
       " 'decided': 878,\n",
       " 'subject': 879,\n",
       " 'expected': 880,\n",
       " 'became': 881,\n",
       " 'truth': 882,\n",
       " 'free': 883,\n",
       " 'screenplay': 884,\n",
       " 'america': 885,\n",
       " 'difficult': 886,\n",
       " 'killing': 887,\n",
       " 'xa9': 888,\n",
       " 'romance': 889,\n",
       " 'imdb': 890,\n",
       " '20': 891,\n",
       " 'nor': 892,\n",
       " 'dr': 893,\n",
       " 'question': 894,\n",
       " 'street': 895,\n",
       " 'leaves': 896,\n",
       " 'needed': 897,\n",
       " 'baby': 898,\n",
       " 'reading': 899,\n",
       " \"couldn't\": 900,\n",
       " 'hot': 901,\n",
       " 'begin': 902,\n",
       " 'meets': 903,\n",
       " 'dog': 904,\n",
       " 'directors': 905,\n",
       " 'credits': 906,\n",
       " 'unless': 907,\n",
       " 'joe': 908,\n",
       " 'write': 909,\n",
       " 'otherwise': 910,\n",
       " 'superb': 911,\n",
       " 'society': 912,\n",
       " 'shame': 913,\n",
       " 'okay': 914,\n",
       " 'situation': 915,\n",
       " 'dramatic': 916,\n",
       " 'memorable': 917,\n",
       " 'weird': 918,\n",
       " 'open': 919,\n",
       " 'badly': 920,\n",
       " 'earlier': 921,\n",
       " 'male': 922,\n",
       " 'meet': 923,\n",
       " 'acted': 924,\n",
       " 'forced': 925,\n",
       " 'emotional': 926,\n",
       " 'sci': 927,\n",
       " 'fi': 928,\n",
       " 'dream': 929,\n",
       " 'laughs': 930,\n",
       " 'writers': 931,\n",
       " 'older': 932,\n",
       " 'realize': 933,\n",
       " 'interested': 934,\n",
       " 'forward': 935,\n",
       " 'footage': 936,\n",
       " 'comment': 937,\n",
       " 'crazy': 938,\n",
       " 'beauty': 939,\n",
       " 'fantasy': 940,\n",
       " 'deep': 941,\n",
       " 'whom': 942,\n",
       " 'sounds': 943,\n",
       " 'plus': 944,\n",
       " 'monster': 945,\n",
       " 'directing': 946,\n",
       " 'keeps': 947,\n",
       " 'ask': 948,\n",
       " 'development': 949,\n",
       " 'features': 950,\n",
       " 'air': 951,\n",
       " 'mess': 952,\n",
       " 'quickly': 953,\n",
       " 'mark': 954,\n",
       " 'd': 955,\n",
       " 'creepy': 956,\n",
       " 'box': 957,\n",
       " 'perfectly': 958,\n",
       " 'towards': 959,\n",
       " 'worked': 960,\n",
       " 'setting': 961,\n",
       " 'result': 962,\n",
       " 'cheesy': 963,\n",
       " 'unique': 964,\n",
       " 'e': 965,\n",
       " 'brings': 966,\n",
       " 'plenty': 967,\n",
       " 'hands': 968,\n",
       " 'effect': 969,\n",
       " 'total': 970,\n",
       " 'previous': 971,\n",
       " 'jane': 972,\n",
       " 'girlfriend': 973,\n",
       " 'fire': 974,\n",
       " 'personal': 975,\n",
       " 'rate': 976,\n",
       " 'bill': 977,\n",
       " 'incredibly': 978,\n",
       " 'business': 979,\n",
       " 'leading': 980,\n",
       " 'joke': 981,\n",
       " 'admit': 982,\n",
       " 'casting': 983,\n",
       " 'appear': 984,\n",
       " 'background': 985,\n",
       " 'powerful': 986,\n",
       " 'apart': 987,\n",
       " 'present': 988,\n",
       " 'telling': 989,\n",
       " 'christmas': 990,\n",
       " 'meant': 991,\n",
       " 'potential': 992,\n",
       " 'battle': 993,\n",
       " 'create': 994,\n",
       " 'break': 995,\n",
       " 'hardly': 996,\n",
       " 'return': 997,\n",
       " 'era': 998,\n",
       " \"you'll\": 999,\n",
       " 'masterpiece': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                1568      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                330       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161909 (632.46 KB)\n",
      "Trainable params: 161909 (632.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn = Sequential([\n",
    "Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "SimpleRNN(32),\n",
    "Dense(10,activation='relu'),\n",
    "Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 12:53:49.656580: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node binary_crossentropy/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/tmp/ipykernel_46680/2807780608.py\", line 1, in <module>\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/losses.py\", line 2521, in binary_crossentropy\n\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_102288]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model_rnn\u001b[38;5;241m.\u001b[39mfit(padded, training_labels_final, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,validation_data \u001b[38;5;241m=\u001b[39m (testing_padded, testing_labels_final) )\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node binary_crossentropy/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/tmp/ipykernel_46680/2807780608.py\", line 1, in <module>\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/home/dai/anaconda3/envs/NLP/lib/python3.11/site-packages/keras/src/losses.py\", line 2521, in binary_crossentropy\n\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_102288]"
     ]
    }
   ],
   "source": [
    "history = model_rnn.fit(padded, training_labels_final, epochs=3, validation_data = (testing_padded, testing_labels_final) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 5ms/step - loss: 3.2574 - accuracy: 0.0400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.257401704788208, 0.03999999910593033]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x, y_new, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 596ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = 'A'\n",
    "a = char_to_int[new]\n",
    "a = np.array(a)\n",
    "a = np.reshape(a, (1,1,1))\n",
    "a.shape\n",
    "\n",
    "x_test = a / 25\n",
    "x_test\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "pred = pred.argmax(axis=1)\n",
    "int_to_char[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(char):\n",
    "    a = char_to_int[char]\n",
    "    a = np.array(a)\n",
    "    a = np.reshape(a, (1,1,1))\n",
    "    x_test = a / 25\n",
    "    pred = model.predict(x_test)\n",
    "    pred = pred.argmax(axis=1)\n",
    "    return int_to_char[pred[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Z'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(len(alphabets)-seq_length):\n",
    "    seq_in = alphabets[i:i+seq_length]\n",
    "    seq_out = alphabets[i+seq_length]\n",
    "    x.append([char_to_int[char] for char in seq_in])\n",
    "    y.append([char_to_int[seq_out]])\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2],\n",
       " [1, 2, 3],\n",
       " [2, 3, 4],\n",
       " [3, 4, 5],\n",
       " [4, 5, 6],\n",
       " [5, 6, 7],\n",
       " [6, 7, 8],\n",
       " [7, 8, 9],\n",
       " [8, 9, 10],\n",
       " [9, 10, 11],\n",
       " [10, 11, 12],\n",
       " [11, 12, 13],\n",
       " [12, 13, 14],\n",
       " [13, 14, 15],\n",
       " [14, 15, 16],\n",
       " [15, 16, 17],\n",
       " [16, 17, 18],\n",
       " [17, 18, 19],\n",
       " [18, 19, 20],\n",
       " [19, 20, 21],\n",
       " [20, 21, 22],\n",
       " [21, 22, 23],\n",
       " [22, 23, 24]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 3, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.reshape(x, (23,seq_length,1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  ],\n",
       "        [0.04],\n",
       "        [0.08]],\n",
       "\n",
       "       [[0.04],\n",
       "        [0.08],\n",
       "        [0.12]],\n",
       "\n",
       "       [[0.08],\n",
       "        [0.12],\n",
       "        [0.16]],\n",
       "\n",
       "       [[0.12],\n",
       "        [0.16],\n",
       "        [0.2 ]],\n",
       "\n",
       "       [[0.16],\n",
       "        [0.2 ],\n",
       "        [0.24]],\n",
       "\n",
       "       [[0.2 ],\n",
       "        [0.24],\n",
       "        [0.28]],\n",
       "\n",
       "       [[0.24],\n",
       "        [0.28],\n",
       "        [0.32]],\n",
       "\n",
       "       [[0.28],\n",
       "        [0.32],\n",
       "        [0.36]],\n",
       "\n",
       "       [[0.32],\n",
       "        [0.36],\n",
       "        [0.4 ]],\n",
       "\n",
       "       [[0.36],\n",
       "        [0.4 ],\n",
       "        [0.44]],\n",
       "\n",
       "       [[0.4 ],\n",
       "        [0.44],\n",
       "        [0.48]],\n",
       "\n",
       "       [[0.44],\n",
       "        [0.48],\n",
       "        [0.52]],\n",
       "\n",
       "       [[0.48],\n",
       "        [0.52],\n",
       "        [0.56]],\n",
       "\n",
       "       [[0.52],\n",
       "        [0.56],\n",
       "        [0.6 ]],\n",
       "\n",
       "       [[0.56],\n",
       "        [0.6 ],\n",
       "        [0.64]],\n",
       "\n",
       "       [[0.6 ],\n",
       "        [0.64],\n",
       "        [0.68]],\n",
       "\n",
       "       [[0.64],\n",
       "        [0.68],\n",
       "        [0.72]],\n",
       "\n",
       "       [[0.68],\n",
       "        [0.72],\n",
       "        [0.76]],\n",
       "\n",
       "       [[0.72],\n",
       "        [0.76],\n",
       "        [0.8 ]],\n",
       "\n",
       "       [[0.76],\n",
       "        [0.8 ],\n",
       "        [0.84]],\n",
       "\n",
       "       [[0.8 ],\n",
       "        [0.84],\n",
       "        [0.88]],\n",
       "\n",
       "       [[0.84],\n",
       "        [0.88],\n",
       "        [0.92]],\n",
       "\n",
       "       [[0.88],\n",
       "        [0.92],\n",
       "        [0.96]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x / 25\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "Dense(25, input_shape=(3,1), activation='relu'))\n",
    "LSTM(128))\n",
    "Dense(26, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "23/23 [==============================] - 3s 7ms/step - loss: 3.2810 - accuracy: 0.0435\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.2468 - accuracy: 0.0435\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 3.2288 - accuracy: 0.0435\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.2033 - accuracy: 0.0435\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 3.1661 - accuracy: 0.0435\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.1211 - accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.0717 - accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.0225 - accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.9676 - accuracy: 0.0435\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.9092 - accuracy: 0.0435\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.8621 - accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.7841 - accuracy: 0.0870\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.7134 - accuracy: 0.0435\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.6247 - accuracy: 0.0435\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.5702 - accuracy: 0.1304\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.4914 - accuracy: 0.0435\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.4583 - accuracy: 0.0870\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.3936 - accuracy: 0.0435\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.3166 - accuracy: 0.1304\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.2361 - accuracy: 0.1304\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.1922 - accuracy: 0.0435\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.1556 - accuracy: 0.1739\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.0598 - accuracy: 0.0435\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.9979 - accuracy: 0.2174\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.9535 - accuracy: 0.2609\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.9343 - accuracy: 0.1304\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.8580 - accuracy: 0.1739\n",
      "Epoch 28/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.8349 - accuracy: 0.1739\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.8082 - accuracy: 0.1739\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.8133 - accuracy: 0.1739\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6978 - accuracy: 0.2609\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.7018 - accuracy: 0.2174\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6283 - accuracy: 0.4348\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6099 - accuracy: 0.2609\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6565 - accuracy: 0.1739\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6580 - accuracy: 0.2609\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4955 - accuracy: 0.3913\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5340 - accuracy: 0.2174\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4986 - accuracy: 0.3478\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4696 - accuracy: 0.4348\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4168 - accuracy: 0.4348\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3731 - accuracy: 0.4783\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3371 - accuracy: 0.5652\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3298 - accuracy: 0.5217\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3095 - accuracy: 0.3913\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2614 - accuracy: 0.5217\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2838 - accuracy: 0.4783\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2258 - accuracy: 0.6957\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2181 - accuracy: 0.4783\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.1819 - accuracy: 0.6087\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2345 - accuracy: 0.6087\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.1722 - accuracy: 0.6522\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2065 - accuracy: 0.5217\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3976 - accuracy: 0.3478\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3555 - accuracy: 0.3913\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.1399 - accuracy: 0.5652\n",
      "Epoch 57/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.1104 - accuracy: 0.6087\n",
      "Epoch 58/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0866 - accuracy: 0.6087\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0379 - accuracy: 0.7391\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0186 - accuracy: 0.7391\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0437 - accuracy: 0.6522\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9763 - accuracy: 0.7391\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9801 - accuracy: 0.6957\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0041 - accuracy: 0.6957\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9682 - accuracy: 0.6957\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9569 - accuracy: 0.7391\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9357 - accuracy: 0.6087\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9487 - accuracy: 0.6087\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0197 - accuracy: 0.6522\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.1323 - accuracy: 0.4348\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.1087 - accuracy: 0.3913\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9332 - accuracy: 0.6087\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8613 - accuracy: 0.7391\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9837 - accuracy: 0.6087\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9904 - accuracy: 0.5217\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0152 - accuracy: 0.5652\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9656 - accuracy: 0.6087\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9855 - accuracy: 0.6087\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0624 - accuracy: 0.5652\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8342 - accuracy: 0.8261\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7935 - accuracy: 0.8261\n",
      "Epoch 82/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8220 - accuracy: 0.7391\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7585 - accuracy: 0.7826\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8318 - accuracy: 0.6957\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8886 - accuracy: 0.6087\n",
      "Epoch 86/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7682 - accuracy: 0.8261\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8675 - accuracy: 0.5652\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8377 - accuracy: 0.6957\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8659 - accuracy: 0.6522\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9291 - accuracy: 0.5217\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7342 - accuracy: 0.8261\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.8696\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6776 - accuracy: 0.7391\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6419 - accuracy: 0.9565\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6231 - accuracy: 0.9130\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6398 - accuracy: 0.8696\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6049 - accuracy: 0.9130\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6151 - accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6117 - accuracy: 0.8696\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7363 - accuracy: 0.6957\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7200 - accuracy: 0.7826\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7561 - accuracy: 0.8261\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7000 - accuracy: 0.6957\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8965 - accuracy: 0.4783\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8592 - accuracy: 0.6522\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.1626 - accuracy: 0.4783\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9766 - accuracy: 0.6522\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8626 - accuracy: 0.6522\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7433 - accuracy: 0.7391\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5688 - accuracy: 0.9565\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5396 - accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5270 - accuracy: 0.9565\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5194 - accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5206 - accuracy: 0.9565\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5102 - accuracy: 0.9565\n",
      "Epoch 116/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.9565\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5197 - accuracy: 0.9130\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5248 - accuracy: 0.9130\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.8261\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.6957\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6030 - accuracy: 0.8696\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5168 - accuracy: 0.9565\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5861 - accuracy: 0.8261\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6097 - accuracy: 0.7391\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5469 - accuracy: 0.8261\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5039 - accuracy: 0.8696\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4689 - accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4603 - accuracy: 0.9130\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4534 - accuracy: 0.9130\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5149 - accuracy: 0.8261\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5644 - accuracy: 0.7826\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6279 - accuracy: 0.8261\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5440 - accuracy: 0.9130\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5326 - accuracy: 0.8696\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4852 - accuracy: 0.8696\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3935 - accuracy: 0.9565\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3926 - accuracy: 0.9565\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3850 - accuracy: 0.9565\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3764 - accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4630 - accuracy: 1.0000\n",
      "Epoch 141/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5579 - accuracy: 0.7826\n",
      "Epoch 142/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5421 - accuracy: 0.8261\n",
      "Epoch 143/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7209 - accuracy: 0.6957\n",
      "Epoch 144/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.0015 - accuracy: 0.6087\n",
      "Epoch 145/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.0478 - accuracy: 0.5652\n",
      "Epoch 146/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.9491 - accuracy: 0.6087\n",
      "Epoch 147/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5496 - accuracy: 0.9130\n",
      "Epoch 148/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4952 - accuracy: 0.8696\n",
      "Epoch 149/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5537 - accuracy: 0.8696\n",
      "Epoch 150/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5512 - accuracy: 0.8261\n",
      "Epoch 151/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6975 - accuracy: 0.5652\n",
      "Epoch 152/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7715 - accuracy: 0.6522\n",
      "Epoch 153/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8531 - accuracy: 0.6522\n",
      "Epoch 154/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7382 - accuracy: 0.7826\n",
      "Epoch 155/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5654 - accuracy: 0.7826\n",
      "Epoch 156/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4903 - accuracy: 0.8696\n",
      "Epoch 157/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4244 - accuracy: 0.8696\n",
      "Epoch 158/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3954 - accuracy: 0.9130\n",
      "Epoch 159/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3859 - accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3626 - accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3102 - accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3143 - accuracy: 1.0000\n",
      "Epoch 163/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2939 - accuracy: 1.0000\n",
      "Epoch 164/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3159 - accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3071 - accuracy: 1.0000\n",
      "Epoch 166/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2880 - accuracy: 1.0000\n",
      "Epoch 167/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2941 - accuracy: 1.0000\n",
      "Epoch 168/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2985 - accuracy: 0.9565\n",
      "Epoch 169/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3001 - accuracy: 1.0000\n",
      "Epoch 170/1000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3129 - accuracy: 1.0000\n",
      "Epoch 171/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3100 - accuracy: 0.9130\n",
      "Epoch 172/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2791 - accuracy: 1.0000\n",
      "Epoch 173/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2750 - accuracy: 1.0000\n",
      "Epoch 174/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2714 - accuracy: 1.0000\n",
      "Epoch 175/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3452 - accuracy: 0.9130\n",
      "Epoch 176/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3103 - accuracy: 0.9130\n",
      "Epoch 177/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3015 - accuracy: 0.9565\n",
      "Epoch 178/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3407 - accuracy: 0.9130\n",
      "Epoch 179/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4724 - accuracy: 0.8261\n",
      "Epoch 180/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.9313 - accuracy: 0.5652\n",
      "Epoch 181/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.9128 - accuracy: 0.3043\n",
      "Epoch 182/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5465 - accuracy: 0.2609\n",
      "Epoch 183/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5510 - accuracy: 0.7391\n",
      "Epoch 184/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.7391\n",
      "Epoch 185/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4418 - accuracy: 0.8696\n",
      "Epoch 186/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3214 - accuracy: 0.9565\n",
      "Epoch 187/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2845 - accuracy: 1.0000\n",
      "Epoch 188/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2681 - accuracy: 1.0000\n",
      "Epoch 189/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2571 - accuracy: 1.0000\n",
      "Epoch 190/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2564 - accuracy: 1.0000\n",
      "Epoch 191/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2436 - accuracy: 1.0000\n",
      "Epoch 192/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2497 - accuracy: 1.0000\n",
      "Epoch 193/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2421 - accuracy: 1.0000\n",
      "Epoch 194/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2376 - accuracy: 1.0000\n",
      "Epoch 195/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2322 - accuracy: 1.0000\n",
      "Epoch 196/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2399 - accuracy: 1.0000\n",
      "Epoch 197/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2318 - accuracy: 1.0000\n",
      "Epoch 198/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2279 - accuracy: 1.0000\n",
      "Epoch 199/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2234 - accuracy: 1.0000\n",
      "Epoch 200/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2254 - accuracy: 1.0000\n",
      "Epoch 201/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2165 - accuracy: 1.0000\n",
      "Epoch 202/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3502 - accuracy: 0.8696\n",
      "Epoch 203/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2870 - accuracy: 0.9565\n",
      "Epoch 204/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2468 - accuracy: 1.0000\n",
      "Epoch 205/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2419 - accuracy: 0.9565\n",
      "Epoch 206/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2033 - accuracy: 1.0000\n",
      "Epoch 207/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2031 - accuracy: 1.0000\n",
      "Epoch 208/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2120 - accuracy: 1.0000\n",
      "Epoch 209/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2007 - accuracy: 1.0000\n",
      "Epoch 210/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1910 - accuracy: 1.0000\n",
      "Epoch 211/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1998 - accuracy: 0.9565\n",
      "Epoch 212/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2308 - accuracy: 0.9130\n",
      "Epoch 213/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3978 - accuracy: 0.7826\n",
      "Epoch 214/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3272 - accuracy: 0.8696\n",
      "Epoch 215/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5747 - accuracy: 0.8261\n",
      "Epoch 216/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7360 - accuracy: 0.6087\n",
      "Epoch 217/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4153 - accuracy: 0.8696\n",
      "Epoch 218/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5151 - accuracy: 0.7826\n",
      "Epoch 219/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.4689 - accuracy: 0.4783\n",
      "Epoch 220/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.0081 - accuracy: 0.2609\n",
      "Epoch 221/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5258 - accuracy: 0.3478\n",
      "Epoch 222/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3508 - accuracy: 0.4783\n",
      "Epoch 223/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5059 - accuracy: 0.3913\n",
      "Epoch 224/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6496 - accuracy: 0.7391\n",
      "Epoch 225/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3736 - accuracy: 1.0000\n",
      "Epoch 226/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3544 - accuracy: 0.9130\n",
      "Epoch 227/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2835 - accuracy: 1.0000\n",
      "Epoch 228/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3038 - accuracy: 0.9565\n",
      "Epoch 229/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2947 - accuracy: 0.9565\n",
      "Epoch 230/1000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2515 - accuracy: 1.0000\n",
      "Epoch 231/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2284 - accuracy: 1.0000\n",
      "Epoch 232/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2125 - accuracy: 1.0000\n",
      "Epoch 233/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1993 - accuracy: 1.0000\n",
      "Epoch 234/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2287 - accuracy: 0.9565\n",
      "Epoch 235/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1997 - accuracy: 1.0000\n",
      "Epoch 236/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1903 - accuracy: 1.0000\n",
      "Epoch 237/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1864 - accuracy: 1.0000\n",
      "Epoch 238/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1805 - accuracy: 1.0000\n",
      "Epoch 239/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1799 - accuracy: 1.0000\n",
      "Epoch 240/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1787 - accuracy: 1.0000\n",
      "Epoch 241/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1743 - accuracy: 1.0000\n",
      "Epoch 242/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1730 - accuracy: 1.0000\n",
      "Epoch 243/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1721 - accuracy: 1.0000\n",
      "Epoch 244/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1838 - accuracy: 1.0000\n",
      "Epoch 245/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1932 - accuracy: 1.0000\n",
      "Epoch 246/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1890 - accuracy: 1.0000\n",
      "Epoch 247/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1629 - accuracy: 1.0000\n",
      "Epoch 248/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1678 - accuracy: 1.0000\n",
      "Epoch 249/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 1.0000\n",
      "Epoch 250/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1915 - accuracy: 0.9565\n",
      "Epoch 251/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3125 - accuracy: 0.8696\n",
      "Epoch 252/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8490 - accuracy: 0.6957\n",
      "Epoch 253/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5754 - accuracy: 0.7826\n",
      "Epoch 254/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3428 - accuracy: 0.8696\n",
      "Epoch 255/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4257 - accuracy: 0.8261\n",
      "Epoch 256/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2998 - accuracy: 0.9130\n",
      "Epoch 257/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2340 - accuracy: 0.9565\n",
      "Epoch 258/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1999 - accuracy: 1.0000\n",
      "Epoch 259/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2080 - accuracy: 0.9130\n",
      "Epoch 260/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1863 - accuracy: 1.0000\n",
      "Epoch 261/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 1.0000\n",
      "Epoch 262/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1713 - accuracy: 0.9565\n",
      "Epoch 263/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1564 - accuracy: 1.0000\n",
      "Epoch 264/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1398 - accuracy: 1.0000\n",
      "Epoch 265/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1423 - accuracy: 1.0000\n",
      "Epoch 266/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1508 - accuracy: 1.0000\n",
      "Epoch 267/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1444 - accuracy: 1.0000\n",
      "Epoch 268/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1419 - accuracy: 1.0000\n",
      "Epoch 269/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1639 - accuracy: 1.0000\n",
      "Epoch 270/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1447 - accuracy: 1.0000\n",
      "Epoch 271/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1400 - accuracy: 0.9565\n",
      "Epoch 272/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1509 - accuracy: 1.0000\n",
      "Epoch 273/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1690 - accuracy: 0.9565\n",
      "Epoch 274/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1806 - accuracy: 0.9565\n",
      "Epoch 275/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1402 - accuracy: 1.0000\n",
      "Epoch 276/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1345 - accuracy: 1.0000\n",
      "Epoch 277/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1265 - accuracy: 1.0000\n",
      "Epoch 278/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1528 - accuracy: 0.9565\n",
      "Epoch 279/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 1.0000\n",
      "Epoch 280/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2540 - accuracy: 0.9130\n",
      "Epoch 281/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5097 - accuracy: 0.7391\n",
      "Epoch 282/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4092 - accuracy: 0.6957\n",
      "Epoch 283/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5547 - accuracy: 0.6957\n",
      "Epoch 284/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.2284 - accuracy: 0.5652\n",
      "Epoch 285/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5053 - accuracy: 0.8261\n",
      "Epoch 286/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6040 - accuracy: 0.8696\n",
      "Epoch 287/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2014 - accuracy: 1.0000\n",
      "Epoch 288/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1658 - accuracy: 1.0000\n",
      "Epoch 289/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1615 - accuracy: 1.0000\n",
      "Epoch 290/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9565\n",
      "Epoch 291/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 1.0000\n",
      "Epoch 292/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2319 - accuracy: 0.9130\n",
      "Epoch 293/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5145 - accuracy: 0.7826\n",
      "Epoch 294/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3334 - accuracy: 0.9130\n",
      "Epoch 295/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5078 - accuracy: 0.4348\n",
      "Epoch 296/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.0781 - accuracy: 0.6957\n",
      "Epoch 297/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7606 - accuracy: 0.6522\n",
      "Epoch 298/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8087 - accuracy: 0.6957\n",
      "Epoch 299/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6564 - accuracy: 0.7391\n",
      "Epoch 300/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3828 - accuracy: 0.8696\n",
      "Epoch 301/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4108 - accuracy: 0.8696\n",
      "Epoch 302/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1827 - accuracy: 1.0000\n",
      "Epoch 303/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1392 - accuracy: 1.0000\n",
      "Epoch 304/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1613 - accuracy: 1.0000\n",
      "Epoch 305/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1346 - accuracy: 1.0000\n",
      "Epoch 306/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1274 - accuracy: 1.0000\n",
      "Epoch 307/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1285 - accuracy: 1.0000\n",
      "Epoch 308/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1276 - accuracy: 1.0000\n",
      "Epoch 309/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1143 - accuracy: 1.0000\n",
      "Epoch 310/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1157 - accuracy: 1.0000\n",
      "Epoch 311/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1170 - accuracy: 1.0000\n",
      "Epoch 312/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1146 - accuracy: 1.0000\n",
      "Epoch 313/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1133 - accuracy: 1.0000\n",
      "Epoch 314/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1111 - accuracy: 1.0000\n",
      "Epoch 315/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1029 - accuracy: 1.0000\n",
      "Epoch 316/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1035 - accuracy: 1.0000\n",
      "Epoch 317/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1078 - accuracy: 1.0000\n",
      "Epoch 318/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1036 - accuracy: 1.0000\n",
      "Epoch 319/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1005 - accuracy: 1.0000\n",
      "Epoch 320/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1233 - accuracy: 1.0000\n",
      "Epoch 321/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0967 - accuracy: 1.0000\n",
      "Epoch 322/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1100 - accuracy: 1.0000\n",
      "Epoch 323/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1618 - accuracy: 0.9130\n",
      "Epoch 324/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1357 - accuracy: 0.9565\n",
      "Epoch 325/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 1.0000\n",
      "Epoch 326/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0946 - accuracy: 1.0000\n",
      "Epoch 327/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0928 - accuracy: 1.0000\n",
      "Epoch 328/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0886 - accuracy: 1.0000\n",
      "Epoch 329/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0892 - accuracy: 1.0000\n",
      "Epoch 330/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0896 - accuracy: 1.0000\n",
      "Epoch 331/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 1.0000\n",
      "Epoch 332/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0888 - accuracy: 1.0000\n",
      "Epoch 333/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1030 - accuracy: 1.0000\n",
      "Epoch 334/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1025 - accuracy: 1.0000\n",
      "Epoch 335/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5219 - accuracy: 0.7391\n",
      "Epoch 336/1000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.0057 - accuracy: 0.6522\n",
      "Epoch 337/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5557 - accuracy: 0.5217\n",
      "Epoch 338/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.3042 - accuracy: 0.3043\n",
      "Epoch 339/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.2355 - accuracy: 0.3913\n",
      "Epoch 340/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7181 - accuracy: 0.6522\n",
      "Epoch 341/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2949 - accuracy: 0.9565\n",
      "Epoch 342/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2292 - accuracy: 1.0000\n",
      "Epoch 343/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2004 - accuracy: 1.0000\n",
      "Epoch 344/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 1.0000\n",
      "Epoch 345/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1569 - accuracy: 1.0000\n",
      "Epoch 346/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1360 - accuracy: 1.0000\n",
      "Epoch 347/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1308 - accuracy: 1.0000\n",
      "Epoch 348/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1188 - accuracy: 1.0000\n",
      "Epoch 349/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1100 - accuracy: 1.0000\n",
      "Epoch 350/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1082 - accuracy: 1.0000\n",
      "Epoch 351/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "Epoch 352/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1051 - accuracy: 1.0000\n",
      "Epoch 353/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0975 - accuracy: 1.0000\n",
      "Epoch 354/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0944 - accuracy: 1.0000\n",
      "Epoch 355/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0964 - accuracy: 1.0000\n",
      "Epoch 356/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0902 - accuracy: 1.0000\n",
      "Epoch 357/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1107 - accuracy: 1.0000\n",
      "Epoch 358/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1312 - accuracy: 1.0000\n",
      "Epoch 359/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1947 - accuracy: 0.9130\n",
      "Epoch 360/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2326 - accuracy: 0.9130\n",
      "Epoch 361/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4725 - accuracy: 0.7391\n",
      "Epoch 362/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2050 - accuracy: 0.6522\n",
      "Epoch 363/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6591 - accuracy: 0.5652\n",
      "Epoch 364/1000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3285 - accuracy: 0.5652\n",
      "Epoch 365/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5845 - accuracy: 0.7391\n",
      "Epoch 366/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3892 - accuracy: 0.8696\n",
      "Epoch 367/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2301 - accuracy: 0.9565\n",
      "Epoch 368/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1291 - accuracy: 1.0000\n",
      "Epoch 369/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1118 - accuracy: 1.0000\n",
      "Epoch 370/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1251 - accuracy: 1.0000\n",
      "Epoch 371/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 1.0000\n",
      "Epoch 372/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1238 - accuracy: 0.9565\n",
      "Epoch 373/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0925 - accuracy: 1.0000\n",
      "Epoch 374/1000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0873 - accuracy: 1.0000\n",
      "Epoch 375/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0893 - accuracy: 1.0000\n",
      "Epoch 376/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0841 - accuracy: 1.0000\n",
      "Epoch 377/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0822 - accuracy: 1.0000\n",
      "Epoch 378/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0845 - accuracy: 1.0000\n",
      "Epoch 379/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0874 - accuracy: 1.0000\n",
      "Epoch 380/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0919 - accuracy: 1.0000\n",
      "Epoch 381/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 0.9565\n",
      "Epoch 382/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1133 - accuracy: 1.0000\n",
      "Epoch 383/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1508 - accuracy: 0.9565\n",
      "Epoch 384/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9990 - accuracy: 0.6522\n",
      "Epoch 385/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7395 - accuracy: 0.7826\n",
      "Epoch 386/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4353 - accuracy: 0.8261\n",
      "Epoch 387/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3881 - accuracy: 0.9130\n",
      "Epoch 388/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3820 - accuracy: 0.8696\n",
      "Epoch 389/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2779 - accuracy: 0.8696\n",
      "Epoch 390/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2167 - accuracy: 0.6087\n",
      "Epoch 391/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.9629 - accuracy: 0.7391\n",
      "Epoch 392/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7360 - accuracy: 0.6957\n",
      "Epoch 393/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6808 - accuracy: 0.6957\n",
      "Epoch 394/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2227 - accuracy: 0.9130\n",
      "Epoch 395/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1865 - accuracy: 0.9565\n",
      "Epoch 396/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2415 - accuracy: 0.8696\n",
      "Epoch 397/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7278 - accuracy: 0.6957\n",
      "Epoch 398/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3742 - accuracy: 0.8261\n",
      "Epoch 399/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3567 - accuracy: 0.9130\n",
      "Epoch 400/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5803 - accuracy: 0.7391\n",
      "Epoch 401/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3503 - accuracy: 0.7826\n",
      "Epoch 402/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.8509 - accuracy: 0.6522\n",
      "Epoch 403/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8665 - accuracy: 0.6957\n",
      "Epoch 404/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4918 - accuracy: 0.7826\n",
      "Epoch 405/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2334 - accuracy: 0.9565\n",
      "Epoch 406/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2606 - accuracy: 0.9130\n",
      "Epoch 407/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3023 - accuracy: 0.9130\n",
      "Epoch 408/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3579 - accuracy: 0.9130\n",
      "Epoch 409/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1004 - accuracy: 1.0000\n",
      "Epoch 410/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0936 - accuracy: 1.0000\n",
      "Epoch 411/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0915 - accuracy: 1.0000\n",
      "Epoch 412/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 413/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0834 - accuracy: 1.0000\n",
      "Epoch 414/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 1.0000\n",
      "Epoch 415/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0793 - accuracy: 1.0000\n",
      "Epoch 416/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0790 - accuracy: 1.0000\n",
      "Epoch 417/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0921 - accuracy: 1.0000\n",
      "Epoch 418/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0842 - accuracy: 1.0000\n",
      "Epoch 419/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0738 - accuracy: 1.0000\n",
      "Epoch 420/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0786 - accuracy: 1.0000\n",
      "Epoch 421/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0950 - accuracy: 1.0000\n",
      "Epoch 422/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1600 - accuracy: 0.9130\n",
      "Epoch 423/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0992 - accuracy: 1.0000\n",
      "Epoch 424/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0866 - accuracy: 1.0000\n",
      "Epoch 425/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 1.0000\n",
      "Epoch 426/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 1.0000\n",
      "Epoch 427/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0675 - accuracy: 1.0000\n",
      "Epoch 428/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0668 - accuracy: 1.0000\n",
      "Epoch 429/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0695 - accuracy: 1.0000\n",
      "Epoch 430/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0695 - accuracy: 1.0000\n",
      "Epoch 431/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0782 - accuracy: 1.0000\n",
      "Epoch 432/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0767 - accuracy: 1.0000\n",
      "Epoch 433/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0676 - accuracy: 1.0000\n",
      "Epoch 434/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0684 - accuracy: 1.0000\n",
      "Epoch 435/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0627 - accuracy: 1.0000\n",
      "Epoch 436/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0604 - accuracy: 1.0000\n",
      "Epoch 437/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0621 - accuracy: 1.0000\n",
      "Epoch 438/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0589 - accuracy: 1.0000\n",
      "Epoch 439/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0622 - accuracy: 1.0000\n",
      "Epoch 440/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0589 - accuracy: 1.0000\n",
      "Epoch 441/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0595 - accuracy: 1.0000\n",
      "Epoch 442/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 1.0000\n",
      "Epoch 443/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0572 - accuracy: 1.0000\n",
      "Epoch 444/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 1.0000\n",
      "Epoch 445/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 446/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0583 - accuracy: 1.0000\n",
      "Epoch 447/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0747 - accuracy: 1.0000\n",
      "Epoch 448/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0882 - accuracy: 1.0000\n",
      "Epoch 449/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0683 - accuracy: 1.0000\n",
      "Epoch 450/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1018 - accuracy: 1.0000\n",
      "Epoch 451/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1060 - accuracy: 0.9565\n",
      "Epoch 452/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1968 - accuracy: 0.9565\n",
      "Epoch 453/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1427 - accuracy: 1.0000\n",
      "Epoch 454/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0801 - accuracy: 1.0000\n",
      "Epoch 455/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1063 - accuracy: 0.9565\n",
      "Epoch 456/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0640 - accuracy: 1.0000\n",
      "Epoch 457/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1455 - accuracy: 0.9130\n",
      "Epoch 458/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1760 - accuracy: 0.9565\n",
      "Epoch 459/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3589 - accuracy: 0.9130\n",
      "Epoch 460/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.1825 - accuracy: 0.6087\n",
      "Epoch 461/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0684 - accuracy: 0.6087\n",
      "Epoch 462/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.9806 - accuracy: 0.7391\n",
      "Epoch 463/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.0150 - accuracy: 0.4348\n",
      "Epoch 464/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5713 - accuracy: 0.4783\n",
      "Epoch 465/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.2139 - accuracy: 0.4783\n",
      "Epoch 466/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.0324 - accuracy: 0.6522\n",
      "Epoch 467/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.0041 - accuracy: 0.6522\n",
      "Epoch 468/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8466 - accuracy: 0.7391\n",
      "Epoch 469/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6947 - accuracy: 0.8261\n",
      "Epoch 470/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.5862 - accuracy: 0.8696\n",
      "Epoch 471/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1848 - accuracy: 0.9565\n",
      "Epoch 472/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.9565\n",
      "Epoch 473/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1055 - accuracy: 1.0000\n",
      "Epoch 474/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 475/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0767 - accuracy: 1.0000\n",
      "Epoch 476/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 1.0000\n",
      "Epoch 477/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0703 - accuracy: 1.0000\n",
      "Epoch 478/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0667 - accuracy: 1.0000\n",
      "Epoch 479/1000\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 1.0000\n",
      "Epoch 480/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0626 - accuracy: 1.0000\n",
      "Epoch 481/1000\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0604 - accuracy: 1.0000\n",
      "Epoch 482/1000\n"
     ]
    }
   ],
   "source": [
    "model.fit(x,y_new, epochs=1000, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 5ms/step - loss: 0.1267 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12671154737472534, 1.0]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x, y_new, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq):\n",
    "    a = []\n",
    "    a.append([char_to_int[char] for char in seq])\n",
    "    a = np.array(a)\n",
    "    a = np.reshape(a, (1,3,1))\n",
    "    x_test = a / 25\n",
    "    pred = model.predict(x_test)\n",
    "    pred = pred.argmax(axis=1)\n",
    "    return int_to_char[pred[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('ABC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Z'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('WXY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('LMN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
